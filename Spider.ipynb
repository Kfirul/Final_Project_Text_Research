{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ed2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5097e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_word(response):\n",
    "    word = response.meta['word']\n",
    "\n",
    "    # Extracting lexicons\n",
    "    word['lexicon_0'] = response.xpath('//body/br[2]/following-sibling::text()').get()\n",
    "    if word['lexicon_0'] and word['lexicon_0'].strip() == '':\n",
    "        word['lexicon_0'] = response.xpath('//body/br[3]/following-sibling::text()').get()\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        lex = response.xpath(f'//body/hr[{i}]/following-sibling::text()').get()\n",
    "        if lex and lex.strip() != '':\n",
    "            word[f'lexicon_{i}'] = lex.strip()\n",
    "        else:\n",
    "            word[f'lexicon_{i}'] = None\n",
    "\n",
    "    # Check if the word is a verb (look for \"verb\" in lexicon_0, lexicon_1, lexicon_2, lexicon_3)\n",
    "    verb_form = None\n",
    "    for lexicon_key in ['lexicon_0', 'lexicon_1', 'lexicon_2', 'lexicon_3']:\n",
    "        if word.get(lexicon_key) and 'verb' in word[lexicon_key]:\n",
    "            verb_form = word[lexicon_key].strip().split(' ')[2]  # Extract the form after 'verb'\n",
    "            break\n",
    "\n",
    "    # If a verb form is detected, extract the corresponding translations from <span class=\"mg1\">\n",
    "    if verb_form:\n",
    "        # Extracting only the second and subsequent <span class=\"mg1\">\n",
    "        translation_list = response.xpath(\n",
    "            f'(//span[@class=\"bin\" and text()=\"{verb_form}\"]/following-sibling::span[@class=\"mg1\"])[position()>1]/text()').getall()\n",
    "\n",
    "        # Clean up translations: filter out numbers and empty strings\n",
    "        cleaned_translations = []\n",
    "        for t in translation_list:\n",
    "            t = t.strip()\n",
    "            if t and not re.match(r'^\\d+$', t):  # Exclude pure numbers\n",
    "                cleaned_translations.append(t)\n",
    "\n",
    "        # Add up to 4 cleaned translations to the word dictionary\n",
    "        for i, translation in enumerate(cleaned_translations[:4]):\n",
    "            word[f'meaning_{i}'] = translation\n",
    "\n",
    "        # Fill any remaining meaning columns with None\n",
    "        for i in range(len(cleaned_translations), 4):\n",
    "            word[f'meaning_{i}'] = None\n",
    "    else:\n",
    "        # Extract multiple word meanings for non-verbs (base logic unchanged)\n",
    "        meanings = response.xpath('//span[@class=\"mgP\"]/text()').getall()\n",
    "        for i, meaning in enumerate(meanings):\n",
    "            if i < 4:\n",
    "                word[f'meaning_{i}'] = meaning.strip()\n",
    "\n",
    "        # Fill any remaining meaning columns with None\n",
    "        for i in range(len(meanings), 4):\n",
    "            word[f'meaning_{i}'] = None\n",
    "\n",
    "    yield word\n",
    "\n",
    "\n",
    "class CalWordSpider(scrapy.Spider):\n",
    "    name = 'cal'\n",
    "    start_urls = [\n",
    "        'https://cal.huc.edu/get_a_chapter.php?file=71009'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for el in response.css('tr > td:nth-child(2) > a'):\n",
    "            word = {\n",
    "                'text': el.css('::text').get(),\n",
    "                'url': el.xpath('@href').get()\n",
    "            }\n",
    "            yield scrapy.Request(\n",
    "                url=f\"https://cal.huc.edu/{word['url']}\",\n",
    "                meta={'word': word},\n",
    "                callback=parse_word\n",
    "            )\n",
    "\n",
    "\n",
    "# Run the spider\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEEDS': {\n",
    "        'Masekhet_tan_checkk.json': {\n",
    "            'format': 'json',\n",
    "            'overwrite': True\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(CalWordSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
